{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Survival.Utils import load_val_data\n",
    "from Survival.Utils import load_score_containers\n",
    "from Survival.Utils import calc_scores\n",
    "from Survival.Utils import filename_generator\n",
    "\n",
    "from Survival.KNNKaplanMeier import KNNKaplanMeier\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dataset: ich\n",
      "---------------------------------------------\n",
      "fold 0\n",
      "1106 16:44:48\n",
      "[LOG]Reading train&test csv...\n",
      "[LOG]Process df row by row and update dict.\n",
      "[LOG]Reading patient list, LoS, TUD...\n",
      "[LOG]Identify one-hot encoded event, extract 'true event' and value.\n",
      "[LOG]Count occurance for each event; Remove 'na' in value.\n",
      "[LOG]Feature filtering. Remove events that occurred for less than 7 times.\n",
      "[LOG]Number of events that ever occurred: 1896\n",
      "[LOG]Number of events that occurred for more than 7 times: 817\n",
      "[LOG]Identify categorical event.\n",
      "[LOG]One-hot encoding categorical event. Scalarize numerical.\n",
      "[LOG]Value filtering. Remove values that occurred for less than 3 times.\n",
      "[LOG]After one-hot encoding, number of feature: 1709\n",
      "[LOG]Among them, number of categorical feature: 1277\n",
      "[LOG]Simplify output dict and check correctness.\n",
      "[LOG]Impute missing value.\n",
      "[LOG]Remove correlated features.\n",
      "    Number of dumplicated featues: 83\n",
      "[LOG]Remove rows that has infinite LoS.\n",
      "---------------------------------------------\n",
      "fold 1\n",
      "1106 16:47:57\n",
      "[LOG]Reading train&test csv...\n",
      "[LOG]Process df row by row and update dict.\n",
      "[LOG]Reading patient list, LoS, TUD...\n",
      "[LOG]Identify one-hot encoded event, extract 'true event' and value.\n",
      "[LOG]Count occurance for each event; Remove 'na' in value.\n",
      "[LOG]Feature filtering. Remove events that occurred for less than 7 times.\n",
      "[LOG]Number of events that ever occurred: 1896\n",
      "[LOG]Number of events that occurred for more than 7 times: 817\n",
      "[LOG]Identify categorical event.\n",
      "[LOG]One-hot encoding categorical event. Scalarize numerical.\n",
      "[LOG]Value filtering. Remove values that occurred for less than 3 times.\n",
      "[LOG]After one-hot encoding, number of feature: 1709\n",
      "[LOG]Among them, number of categorical feature: 1277\n",
      "[LOG]Simplify output dict and check correctness.\n",
      "[LOG]Impute missing value.\n",
      "[LOG]Remove correlated features.\n",
      "    Number of dumplicated featues: 94\n",
      "[LOG]Remove rows that has infinite LoS.\n",
      "---------------------------------------------\n",
      "fold 2\n",
      "1106 16:51:20\n",
      "[LOG]Reading train&test csv...\n",
      "[LOG]Process df row by row and update dict.\n",
      "[LOG]Reading patient list, LoS, TUD...\n",
      "[LOG]Identify one-hot encoded event, extract 'true event' and value.\n",
      "[LOG]Count occurance for each event; Remove 'na' in value.\n",
      "[LOG]Feature filtering. Remove events that occurred for less than 7 times.\n",
      "[LOG]Number of events that ever occurred: 1896\n",
      "[LOG]Number of events that occurred for more than 7 times: 817\n",
      "[LOG]Identify categorical event.\n",
      "[LOG]One-hot encoding categorical event. Scalarize numerical.\n",
      "[LOG]Value filtering. Remove values that occurred for less than 3 times.\n",
      "[LOG]After one-hot encoding, number of feature: 1709\n",
      "[LOG]Among them, number of categorical feature: 1277\n",
      "[LOG]Simplify output dict and check correctness.\n",
      "[LOG]Impute missing value.\n",
      "[LOG]Remove correlated features.\n",
      "    Number of dumplicated featues: 83\n",
      "[LOG]Remove rows that has infinite LoS.\n",
      "---------------------------------------------\n",
      "fold 3\n",
      "1106 16:54:02\n",
      "[LOG]Reading train&test csv...\n",
      "[LOG]Process df row by row and update dict.\n",
      "[LOG]Reading patient list, LoS, TUD...\n",
      "[LOG]Identify one-hot encoded event, extract 'true event' and value.\n",
      "[LOG]Count occurance for each event; Remove 'na' in value.\n",
      "[LOG]Feature filtering. Remove events that occurred for less than 7 times.\n",
      "[LOG]Number of events that ever occurred: 1896\n",
      "[LOG]Number of events that occurred for more than 7 times: 817\n",
      "[LOG]Identify categorical event.\n",
      "[LOG]One-hot encoding categorical event. Scalarize numerical.\n",
      "[LOG]Value filtering. Remove values that occurred for less than 3 times.\n",
      "[LOG]After one-hot encoding, number of feature: 1709\n",
      "[LOG]Among them, number of categorical feature: 1277\n",
      "[LOG]Simplify output dict and check correctness.\n",
      "[LOG]Impute missing value.\n",
      "[LOG]Remove correlated features.\n",
      "    Number of dumplicated featues: 84\n",
      "[LOG]Remove rows that has infinite LoS.\n",
      "---------------------------------------------\n",
      "fold 4\n",
      "1106 16:56:44\n",
      "[LOG]Reading train&test csv...\n",
      "[LOG]Process df row by row and update dict.\n",
      "[LOG]Reading patient list, LoS, TUD...\n",
      "[LOG]Identify one-hot encoded event, extract 'true event' and value.\n",
      "[LOG]Count occurance for each event; Remove 'na' in value.\n",
      "[LOG]Feature filtering. Remove events that occurred for less than 7 times.\n",
      "[LOG]Number of events that ever occurred: 1896\n",
      "[LOG]Number of events that occurred for more than 7 times: 817\n",
      "[LOG]Identify categorical event.\n",
      "[LOG]One-hot encoding categorical event. Scalarize numerical.\n",
      "[LOG]Value filtering. Remove values that occurred for less than 3 times.\n",
      "[LOG]After one-hot encoding, number of feature: 1709\n",
      "[LOG]Among them, number of categorical feature: 1277\n",
      "[LOG]Simplify output dict and check correctness.\n",
      "[LOG]Impute missing value.\n",
      "[LOG]Remove correlated features.\n",
      "    Number of dumplicated featues: 97\n",
      "[LOG]Remove rows that has infinite LoS.\n"
     ]
    }
   ],
   "source": [
    "# get the parameters\n",
    "n_neighbors = [3, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200]\n",
    "pca_flags = [True, False]\n",
    "dataset_idxs = [1] # 0: \"pancreatitis\", 1: \"ich\", 2: \"sepsis\"\n",
    "\n",
    "train_dfs, test_dfs, dataset_names = load_val_data(dataset_idxs, verbose=True, data_path=\"../../dataset/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = dataset_names[0]\n",
    "n_neighbor = 80\n",
    "pca_flag = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n",
      "0 1 2 3 4 \n",
      "[LOG] avg. concordance: 0.5743002207505519\n",
      "[LOG] avg. ipec: 0.1832287420183661\n"
     ]
    }
   ],
   "source": [
    "concordances = []\n",
    "ipecs = []\n",
    "\n",
    "for i in range(10):\n",
    "    tmp_concordances = []\n",
    "    tmp_ipecs = []\n",
    "\n",
    "    for index, cur_train in enumerate(train_dfs[dataset_name]):\n",
    "        print(index, end=\" \")\n",
    "        model = KNNKaplanMeier(n_neighbors=n_neighbor, \n",
    "            pca_flag=pca_flag, n_components=20)\n",
    "        model.fit(cur_train, duration_col='LOS', event_col='OUT')\n",
    "        concordance, ipec_score = \\\n",
    "            calc_scores(model, cur_train, test_dfs[dataset_name][index])\n",
    "\n",
    "        tmp_concordances.append(concordance)\n",
    "        tmp_ipecs.append(ipec_score)\n",
    "    print()\n",
    "\n",
    "    avg_concordance = np.average(tmp_concordances)\n",
    "    avg_ipec = np.average(tmp_ipecs)\n",
    "    print(\"[LOG] avg. concordance:\", avg_concordance)\n",
    "    print(\"[LOG] avg. ipec:\", avg_ipec)\n",
    "    concordances.append(avg_concordance)\n",
    "    ipecs.append(avg_ipec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
