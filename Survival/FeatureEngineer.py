import pandas as pd
import numpy as np
from scipy import stats
import re
from collections import defaultdict
from collections import Counter
from fancyimpute import SoftImpute

"""
sample usage:
fe = FeatureEngineer(verbose=True)
fe.get_diseases_list()
fe.load_data(src_idx=0, file_prefix="", low_freq_event_thd=0.0003, 
    low_freq_value_thd=0.0001)
"""

class FeatureEngineer():
    def __init__(self, verbose=True, data_path=None):
        """
        :param verbose: print out execution steps to console.
        :param data_path: the/path/to/the/preprocessed/, the `preprocessed`
        folder is generated by George's code.
        """

        # Type of disease (dataset)
        self.SOURCES = ["pancreatitis", "ich", "sepsis"]
        # Note: across the whole project (not only this file), we always use
        # index (digit) to refer to different diseases. According to the
        # definition here, 0 -> "pancreatitis", 1 -> "ich", 2 -> "sepsis"

        # Update the data path into a file path template
        if data_path is None:
            self.DATA_PATH = \
                "/media/latte/npsurvival/preprocessed/" + \
                "{SOURCE}_forecast_icu_los/{FILENAME}"
        else:
            self.DATA_PATH = data_path + "{SOURCE}_forecast_icu_los/{FILENAME}"
        self.verbose = verbose

    def _value_cleaning(self, piece_value, piece_event):
        """
        Clean value for each row. This include parsing string into number (if
        applicable), unifying NaN values into "na" (hard-coded), and unifying
        duplicated representation of the same value (hard-coded)

        :param piece_value: the original value (string)
        :param piece_event: the name of the event (key)
        :return: the parsed value (float or string)
        """

        # Hard-coded "na" representation found in the dataset (it is not
        # guaranteed to be comprehensive)
        na_set = {"", "error", "notdone", "none", "**_info_not_available_**",
                  'unable_t_report', "unable_to_report",
                  'error,unable_to_report', "discarded",
                  'computer_network_failure._test_not_resulted.',
                  "error_specimen_clotted", "specimen_clotted",
                  'disregard_result', "test_not_done", 'unable_to_determine',
                  'unable_to_determine:', "unknown", 'unable_to_repoet',
                  'unable_to_repert', 'not_reported', 'specimen_clottted',
                  'not_done', "unable", "unable_to_repot", 'spec._clotted',
                  'spec.clotted', 'unable_to_analyze'}

        # Hard-coded duplicated representation for the same value (e.g. "m" and
        # "married")
        dup_value_ref = {
            "chart:marital_status": {'w': 'widowed', 'm': 'married',
                                     's': 'single', 'd': 'divored'},
            "chart:marital_status:na": {'w': 'widowed', 'm': 'married',
                                        's': 'single', 'd': 'divored'},
            "lab:urine:hematology:bacteria": {'o': 'occ', 'm': 'mod',
                                              'f': 'few'},
            "lab:urine:hematology:glucose": {'tr': 'na', 'neg': -1},
            "lab:urine:hematology:ketone": {'tr': 'na', 'neg': -1},
            "lab:urine:hematology:protein": {'tr': 'na', 'neg': -1},
            "lab:urine:hematology:urine_appearance": {"slcloudy": "slcldy"},
            "lab:urine:hematology:urine_color": {'y': 'yellow',
                                                 'yel': 'yellow', 's': 'straw',
                                                 'drkamber': 'dkamber',
                                                 'dkambe': 'dkamber',
                                                 'dkamb': 'dkamber',
                                                 'amb': 'amber'},
            "lab:urine:hematology:urobilinogen": {'neg': -1},
            "lab:blood:chemistry:ethanol": {'neg': -1},
            "lab:blood:chemistry:acetaminophen": {'neg': -1},
            "lab:blood:chemistry:salicylate": {'neg': -1}
        }

        # Hard-coded for some categorical event, change the numerical
        # representation into the string one (which is much more common in
        # the dataset)
        zero_one_special = {
            "lab:urine:hematology:bacteria": {0: "zero"},
            "chart:parameters_checked:na": {1: 'yes'},
            "lab:blood:hematology:anisocytosis": {1: '1'},
            "lab:blood:hematology:poikilocytosis": {1: '1'},
            "lab:urine:hematology:yeast": {0: 'zero'}
        }

        # Convert value from str into float, if possible
        try:
            piece_value = float(piece_value)
            # Change 0 / 1 into string if necessary (if the event is actually
            # a categorical event)
            if piece_value == 1 or piece_value == 0:
                if piece_event in zero_one_special:
                    piece_value = zero_one_special[piece_event][piece_value]
        except:
            # Handle three typical scenarios when a numerical value (n) cannot
            # be converted into a number:
            # n1-n2 => (n1+n2)/2
            # >n1 | <=n1 | ... => n1
            # n1 cm => n1
            piece_value = piece_value.replace(" ", "_")
            pattern_1 = r'.*([0-9]+)_?-_?([0-9]+).*'
            pattern_2 = r'[a-z_:]*(<|>|<=|>=|less_than|greater_than' + \
                        r'|greater_thn|less_thn)_?([0-9]+).*'
            pattern_3 = r'([0-9]+)_?(cm|mm|grams|gmmag|mgso4|gm_mag|gmg+|o1b)'
            if re.match(pattern_1, piece_value):
                # "n1-n2" => (n1+n2)/2
                result = re.match(pattern_1, piece_value)
                piece_value = (float(result[1]) + float(result[2])) / 2
            elif re.match(pattern_2, piece_value):
                # ">n1" | "<=n1" | ... => n1
                result = re.match(pattern_2, piece_value)
                piece_value = float(result[2])
            elif re.match(pattern_3, piece_value):
                # "n1 cm" => n1
                result = re.match(pattern_3, piece_value)
                piece_value = float(result[1])

        # All "na" value will be represented with the unified "na" string,
        # which will be handled later
        if type(piece_value) == float and np.isnan(piece_value):
            piece_value = "na"

        # After previous steps, all numerical values should have already been
        # cleaned
        if type(piece_value) == str:
            piece_value = piece_value.replace(" ", "_")
            # unify "na" representation
            if piece_value in na_set or \
                    re.match(r'unable_to.*due_to.*', piece_value):
                piece_value = "na"
            # unify duplicated representation
            elif piece_event in dup_value_ref:
                if piece_value in dup_value_ref[piece_event]:
                    piece_value = dup_value_ref[piece_event][piece_value]
            else:
                pattern = r".*(disregard_previous|error_previously" + \
                          r"_reported|disregard_result).*"
                if re.match(pattern, piece_value):
                    piece_value = "na"

        return piece_value

    def _scalarize_numerical_value_list(self, value_list):
        """
        For each patient, one event can have multiple values. If the event is
        numerical, we need to get a scalar out of the value list

        :param value_list: the list of numerical values
        :return: the scalar value
        """
        # use latest information
        return value_list[-1]
        # use the mean of all information
        # return np.mean(value_list)

    def _process_event_ends_with_colon(self, target_event, source_df):
        """
        Process the colon-ending events.
        The reason for this is that on a later step, we will split by ":" and
        check the last token.
        :param target_event:
        :param source_df:
        :return:
        """

        # look for derived event that contains the target event
        # e.g. `chart:admit_wt:kg` is a derived event of `chart:admit_wt:`
        derive_events = []
        for event in source_df["event"].unique():
            if target_event in event and target_event != event:
                derive_events.append(event)

        # get value list of the target_event
        val_list = list(source_df[source_df["event"] == target_event]["value"])
        val_list = self._remove_na_in_list(val_list)

        if len(derive_events) == 0:
            # if no other event that contains the target
            if len(val_list) == 0:
                # if all values are 'na' or np.nan or None or "",
                # the event will be processed as a one-hot-encoded event in the
                # later step.
                # In this case, we add 'na' after the original target event
                # e.g. 'chart:alt:' -> 'chart:alt:na'
                output_event = target_event + "na"
            else:
                # the event has a list of values, so it won't be processed as a
                # one-hot-encoded event. i.e. the event_name won't be split.
                # In this case, delete the last ':'
                # e.g. 'chart:alt:' -> 'chart:alt'
                output_event = target_event[:-1]
        else:
            # else, for the target event, there exists derived events
            # the target event is likely to be a special case of the existed
            # derived event, in which case we may want to merge the two events
            output_event = target_event

            # some target event may have multiple derived event
            for derive_event in derive_events:
                derive_val_list = \
                    list(source_df[source_df["event"] == derive_event]["value"])
                derive_val_list = self._remove_na_in_list(derive_val_list)

                # if the target event itself does not have meaningful values
                if len(val_list) == 0:
                    # directly merge
                    output_event = derive_event
                    val_list.extend(derive_val_list)
                elif len(derive_val_list) == 0:
                    pass
                else:
                    # if two events both have some valid values, check if they
                    # comes from the same distribution
                    if self._check_same_distribution(val_list, derive_val_list):
                        # merge
                        if output_event in derive_event:
                            output_event = derive_event
                        val_list.extend(derive_val_list)
                    else:
                        pass

            # After all the merging, re-check if the event name ends with ":"
            if output_event.endswith(":"):
                if len(val_list) == 0:  # all values are 'na'|np.nan|None|""
                    output_event = output_event + "na"
                else:  # the event_name won't be split later
                    output_event = output_event[:-1]

        return output_event

    def _from_df_to_dict(self, df):
        """
        Process the read in dataframe:
        1. data cleaning (both event-name and values)
        2. change the dataframe into a dict
        Each line of the read in dataframe contains:
        patient_id, event_name, value

        :param df: the concatenated dataframe of both train and test dataset
        :return:
        """

        # some hard-coded event names merges
        event_change_coded_table = {
            "chart:admit wt": "chart:admit wt:kg",
            "chart:bun": "chart:bun:mg/dl",
            "chart:creatinine": "chart:creatinine:mg/dl",
            "chart:tidal volume (observed)": \
                "chart:tidal volume (observed):ml/b",
            "chart:tidal volume (set)": "chart:tidal volume (set):ml/b"
        }

        event_change_dict = {}
        patient_dict = defaultdict(lambda: defaultdict(list))
        na_event_set = set([])
        non_na_event_set = set([])

        if self.verbose:
            print("[LOG]Process df row by row and update dict.")

        # Traverse the dataframe row by row
        matrix = df.values
        feature_idx = {feature: idx for idx, feature in enumerate(df.columns)}
        for pieces in matrix:
            patient_id = int(float(pieces[feature_idx["patientID"]]))
            piece_event = pieces[feature_idx["event"]]
            # Clean the value based on rules in the `_value_cleaning` function
            piece_value = self._value_cleaning(
                pieces[feature_idx["value"]], piece_event)
            # Process the event name if it ends with ":"
            if piece_event.endswith(":"):
                if piece_event in event_change_dict:
                    # if one changing rule has already been established,
                    # directly use the changing result
                    piece_event = event_change_dict[piece_event]
                else:
                    piece_event = \
                        self._process_event_ends_with_colon(piece_event, df)
                    # remember the established changing rules
                    event_change_dict[pieces[feature_idx["event"]]] = \
                        piece_event

            # some special cases
            if piece_event == "chart:admit wt":
                piece_value = self._pounds_to_kg(piece_value)  # lbs to kg
            if piece_event in event_change_coded_table:
                piece_event = event_change_coded_table[piece_event]
            piece_event = piece_event.replace(" ", "_")

            # At this point, for each patient's each event, there might be a
            # bunch of different values (if there are multiple rows in the
            # original dataframe that belongs to the same patient's same event).
            # Also, at this point, the basic data cleaning part is done.
            patient_dict[patient_id][piece_event].append(piece_value)

            # By the way, find out those events that only have "na" value
            if piece_value == 'na' or \
                    (type(piece_value) == float and np.isnan(piece_value)):
                na_event_set.add(piece_event)
            else:
                non_na_event_set.add(piece_event)

        all_na_event_set = na_event_set - non_na_event_set
        return patient_dict, all_na_event_set

    def _check_categorical(self, value_list, cat_cnt_thd):
        """
        Check if an event is categorical based on its value list.

        Two thresholds are used to identify categorical event:
        The cat_cnt_thd is given as parameter. It is set to be 4 as default. If
        it is greater than the total number of possible values of the target
        event, set it as the total number of values.
        The ratio_thd is 20% of the total number of possible values of the
        target event.
        If the number of categorical values of the target event is smaller than
        either of these two threshold, identify this event as a numerical event.

        :param value_list:
        :param cat_cnt_thd:
        :return:
        """
        categorical_count = 0
        ratio_thd = 0.2 * len(value_list)
        cat_cnt_thd = np.min([cat_cnt_thd, len(value_list)])

        # count the number of string values in the value list
        for value in list(value_list):
            if type(value) == str:
                categorical_count += 1

        # check categorical based on two thresholds. This rules are chosen based
        # on simple experiments and observations.
        if categorical_count < cat_cnt_thd or categorical_count < ratio_thd:
            return False
        return True

    def _corr_feature_filter(self, df, corr_thd=1, method='pearson'):
        """
        Check if there is any pair of features that are strongly correlated to
        each other

        :param df:
        :param corr_thd:
        :param method:
        :return:
        """
        corr_df = df.corr(method=method)
        dup_feature = []
        for index, feature1 in enumerate(corr_df.columns):
            for feature2 in corr_df[feature1].index[index + 1:]:
                if np.abs(corr_df[feature1][feature2]) >= corr_thd and \
                                feature1 != feature2:
                    dup_feature.append(feature2)
        return dup_feature

    def load_data(self, src_idx, file_prefix="", low_freq_event_thd=0.05,
                  low_freq_value_thd=0.05, cat_cnt_thd=4,
                  mask_single_num_value_feature=False, zero_for_non_exist=False,
                  remove_corr_feature=False, remove_inf_time_datapoint=False):
        """
        Extracting data from pre-processed dataset by George;
        Transforming data with proper cleaning and processing;
        Loading data file into python dict.

        @params src_idx        Integer {0,1,2} Code choosing different dataset.
                               [src_idx]=0: "pancreatitis",
                               [src_idx]=1: "ich",
                               [src_idx]=2: "sepsis"
        @params file_prefix="" String. File prefix to choose cross validation
                               dataset or the complete train&test dataset.
                               [file_prefix]="": Use complete train & test
                               [file_prefix]="cross_val/x-xfold_x_": cross-val
        @params low_freq_event_thd=0.05 Float [0, 1], If only less than
                               [low_freq_event_thd] * patients has a certain
                               event, drop this event for simplicity.
                               [low_freq_event_thd]=1: use all events.
        @params low_freq_value_thd=0.05 Float [0, 1], For categorical features,
                               If only less than [low_freq_event_thd] * patients
                               has a certain value, drop this value for
                               simplicity.
                               [low_freq_event_thd]=1: use all events.
        @params cat_cnt_thd=4  Integer [1, inf), If more than [cat_cnt_thd]
                               values of a event is str, then identify this
                               event as categorical.
        @params mask_single_num_value_feature   Boolean, True: If a numerical
                                                event only has one value, mask
                                                it as 0 / 1
        @params zero_for_non_exist              Boolean, True: When one-hot
                                                encoding categorical features,
                                                set 1 if has, 0 if doesnt has
        @params remove_corr_feature             Boolean, True: After missing
                                                value imputation, remove
                                                correlated featuress
        @params remove_inf_time_datapoint       Boolean, True: Remove datapoints
                                                (patients) that have "inf" LOS

        @return patient_dict   dict. {patient_id: {processed_feature: value}}
        @return train_id_set   set. patient_id for training
        @return test_id_set    set. patient_id for training
        @return feature_list   list. all features to use
        @return cat_feature set. categorical feature
        """

        id_set = {}

        ## Reading train&test csv...
        if self.verbose:
            print("[LOG]Reading train&test csv...")
        fname = file_prefix + "{}.csv"
        train_df = pd.read_csv(
            self._get_file_path(src_idx, fname.format("train")),
            header=None, names=["patientID", "event", "value"])
        test_df = pd.read_csv(
            self._get_file_path(src_idx, fname.format("test")),
            header=None, names=["patientID", "event", "value"])
        # All the below feature engineering are done with both train and test
        complete_df = pd.concat([train_df, test_df])

        ## Data cleaning
        patient_dict, all_na_event_set = self._from_df_to_dict(complete_df)

        ## Reading patient list, LoS, TUD...
        if self.verbose:
            print("[LOG]Reading patient list, LoS, TUD...")
        for dataset_tag in ["train", "test"]:
            patient_id_list = \
                self._read_text_file(src_idx, file_prefix, dataset_tag,
                                     "patients")
            los_value_list = \
                self._read_text_file(src_idx, file_prefix, dataset_tag,
                                     "patient_ICU_LoS")
            tud_value_list = \
                self._read_text_file(src_idx, file_prefix, dataset_tag,
                                     "patient_time_until_death_from_in_ICU")
            id_set[dataset_tag] = set(patient_id_list)

            # Update the LOS, OUT, DIE info into the dict
            for index, patient_id in enumerate(patient_id_list):
                patient_dict[patient_id]["LOS"] = [los_value_list[index]]
                # patient_dict[patient_id]["TUD"] = [tud_value_list[index]]
                patient_dict[patient_id]["OUT"] = \
                    [int(los_value_list[index] != np.inf)]
                patient_dict[patient_id]["DIE"] = \
                    [int(tud_value_list[index] != np.inf)]

        ## Processing one-hot-encoded event
        # "One-hot-encoded events": events that have only 'na' value across the
        # whole dataset.
        if self.verbose:
            print("[LOG]Identify one-hot encoded event, " + \
                  "extract 'true event' and value.")
        # process all-na events (extract true event and value)
        na_new_event_set = set([])
        non_na_new_event_set = set([])

        for all_na_event in list(all_na_event_set):
            for patient_id in patient_dict.keys():
                if all_na_event in patient_dict[patient_id]:
                    event_tokens = all_na_event.split(":")
                    # If the all-na event cannot be divided as true event and
                    # value, directly delete this event now
                    # Notice that these events are likely to be meaningful if we
                    # take timestamp into consideration
                    if len(event_tokens) == 1:
                        patient_dict[patient_id][all_na_event] = ["done"]
                    # If the all-na event can be divided into multiple parts, we
                    # that the first few parts (other than the last one) will
                    # form the true event (feature) and the last part is the
                    # value.
                    # e.g. "microbiology:viral_culture:simplex_virus:herpes..."
                    # -> "microbiology:viral_culture:simplex_virus": "herpes..."
                    else:
                        patient_dict[patient_id].pop(all_na_event, None)
                        new_event = ":".join(event_tokens[:-1])
                        new_value = self._value_cleaning(
                            event_tokens[-1], new_event)
                        if new_value == 'na':
                            na_new_event_set.add(new_event)
                        else:
                            non_na_new_event_set.add(new_event)
                        patient_dict[patient_id][new_event].append(new_value)

        # after this step, get the remaining events that have only "na" value
        all_na_new_event_set = na_new_event_set - non_na_new_event_set
        for all_na_event in list(all_na_new_event_set):
            for patient_id in patient_dict:
                if all_na_event in patient_dict[patient_id]:
                    # change the "na" into explicit information: this event
                    # exists for this patient
                    patient_dict[patient_id][all_na_event] = ["exist"]

        ## Up to this point, all events should have at least one non-na value
        ## Thus, the remaining "na" in the dataset does not provide any more
        ## information

        ## Remove remaining 'na' in the dataset and count occurrence of events
        if self.verbose:
            print("[LOG]Count occurrence for each event; Remove 'na' in value.")
        event_per_patient_count = defaultdict(int)
        event_value_dict = defaultdict(set)
        # count number of occurrence for each event, filter out low-freq events
        for patient_id in patient_dict:
            ori_event_list = list(patient_dict[patient_id].keys())
            for event in ori_event_list:
                # Assume that 'na' doesn't provide any information at this point
                # Treat 'na' as missing value -> remove 'na'.
                ori_value_list = patient_dict[patient_id][event]
                value_list_without_na = [x for x in ori_value_list if x != 'na']

                # If the value list of this event of this patient has only 'na',
                # which was removed, then delete this event because there is no
                # information about this event.
                if len(value_list_without_na) == 0:
                    patient_dict[patient_id].pop(event, None)
                    continue

                # Update patient dict.
                patient_dict[patient_id][event] = value_list_without_na

                # Count occurance.
                event_per_patient_count[event] += 1

                # Get all possible value for each event.
                event_value_dict[event] |= set(value_list_without_na)

        ## Filter out low-freq events
        if type(low_freq_event_thd) == float:
            feature_num_thd = int(len(patient_dict) * low_freq_event_thd)
        elif type(low_freq_event_thd) == int:
            feature_num_thd = low_freq_event_thd
        else:
            feature_num_thd = 1
        low_freq_events = set([])
        if self.verbose:
            print("[LOG]Event filtering. Remove events that occurred" + \
                  " for less than", feature_num_thd, "times.")
        if self.verbose:
            print("[LOG]- Number of events that ever occurred:",
                  len(event_per_patient_count))
        # remove events that occurred for less than 0.05 * patient_number times
        for event in event_per_patient_count.keys():
            if event_per_patient_count[event] <= feature_num_thd:
                low_freq_events.add(event)
                event_value_dict.pop(event, None)
        if self.verbose:
            print("[LOG]- Number of events that occurred for more than" + \
                  " {} times: {}".format(feature_num_thd,
                                         len(event_value_dict)))

        ## list events that only has one value and process based on rules:
        #
        # for categorical value, if the value itself does make sense, leave the
        # event as its current status, as during one-hot encoding this will
        # automatically result in a dummy feature indicating 1 or 0.
        # for value that does not make sense, e.g. "see comment", delete the
        # event.
        #
        # for numerical value, this is problematic because this may result in
        # wrong inputation when fill in the missing value. Currently create a
        # mask feature to indicate the existence of this feature, and manually
        # fill in missing value (with 0).
        event_with_one_value = {}
        for event in list(event_value_dict):
            if event in {"OUT", "LOS", "DIE", "TUD"}:
                continue
            if len(event_value_dict[event]) == 1:
                value = list(event_value_dict[event])[0]
                if type(value) == float or type(value) == int:
                    if value == 0:
                        event_with_one_value[event] = "mask"
                    if mask_single_num_value_feature:
                        event_with_one_value[event] = "mask"
                elif value == "see_comments":
                    # hard-coded special case
                    event_with_one_value[event] = "delete"

        for patient_id in patient_dict:
            for event in event_with_one_value:
                if event_with_one_value[event] == "mask":
                    new_event = event + "_existance_flag"
                    # the new event will only have 0 / 1 value
                    new_value = float(event in patient_dict[patient_id])
                    # add the existence flag, remove original one-value event
                    patient_dict[patient_id][new_event] = [new_value]
                    patient_dict[patient_id].pop(event, None)
                    # update the value list of each event
                    event_value_dict[new_event].add(new_value)
                    event_value_dict.pop(event, None)
                elif event_with_one_value[event] == "delete":
                    patient_dict[patient_id].pop(event, None)
                    event_value_dict.pop(event, None)

        ## Identify categorical / numerical event
        # Up to this point, all the cleaning stuff should be almost done. We can
        # try to classify chich events are categorical, and then one-hot encoded
        # them
        if self.verbose:
            print("[LOG]Identify categorical event.")
        all_events = event_value_dict.keys()
        cat_events = set([])

        for event in all_events:
            # Get list of categorical event for ont-hot in the next step   
            if self._check_categorical(event_value_dict[event], cat_cnt_thd):
                cat_events.add(event)

        # For categorical events, combine or delete encoded features if its
        # frequency is too small
        if type(low_freq_value_thd) == float:
            value_num_thd = int(len(patient_dict) * low_freq_value_thd)
        elif type(low_freq_value_thd) == int:
            value_num_thd = low_freq_value_thd
        else:
            value_num_thd = 1

        # count categorical event's value number
        cat_value_count = defaultdict(lambda: Counter())
        for patient_id in patient_dict.keys():
            for event in cat_events:
                if event not in patient_dict[patient_id]:
                    continue
                cat_value_count[event].update(patient_dict[patient_id][event])

        # For each categorical event, some of its values may need to be combined
        # into one value, the "combine-value", because each of the value occurs
        # for too few times
        cat_value_ref = defaultdict(dict)
        for event in cat_events:
            for value in cat_value_count[event]:
                if cat_value_count[event][value] < value_num_thd:
                    cat_value_ref[event][value] = "combine_value"
                else:
                    cat_value_ref[event][value] = value

        if self.verbose:
            print("[LOG]One-hot encoding categorical event. " + \
                  "Scalarize numerical.")
        # for categorical event, ont-hot for each patient
        cat_feature_count = Counter()
        for patient_id in patient_dict.keys():
            for event in all_events:
                if event not in patient_dict[patient_id]:
                    continue
                value_list = patient_dict[patient_id][event]
                if event in cat_events:  # categorical event
                    # For categorical event, one-hot encoding
                    patient_dict[patient_id].pop(event, None)
                    for value in set(value_list):
                        trans_value = cat_value_ref[event][value]
                        new_event = event + "__" + str(trans_value)
                        patient_dict[patient_id][new_event] = 1
                        cat_feature_count[new_event] += 1
                else:  # numerical event
                    # For numerical event, use the lastest info
                    true_value_list = [x for x in value_list if type(x) != str]
                    if len(true_value_list) == 0:
                        patient_dict[patient_id].pop(event, None)
                    else:
                        patient_dict[patient_id][event] = \
                            self._scalarize_numerical_value_list(
                                true_value_list)

        if self.verbose:
            print("[LOG]Value filtering. Remove values that occurred" + \
                  " for less than", value_num_thd, "times.")
        cat_feature = set([])
        for feature in cat_feature_count:
            if cat_feature_count[feature] > value_num_thd:
                cat_feature.add(feature)
                # manually set the dummy feature as 0 if a patient does not
                # have this event-value pair.
                if zero_for_non_exist:
                    for patient_id in patient_dict:
                        if feature not in patient_dict[patient_id]:
                            patient_dict[patient_id][feature] = 0

        feature_set = (all_events - cat_events) | cat_feature

        if self.verbose:
            print("[LOG]After one-hot encoding, number of feature:",
                  len(feature_set))
        if self.verbose:
            print("[LOG]Among them, number of categorical feature:",
                  len(cat_feature))

        if self.verbose:
            print("[LOG]Simplify output dict and check correctness.")
        # check correctness
        for patient_id in patient_dict.keys():
            feature_delete = set(patient_dict[patient_id].keys()) - feature_set
            for feature in feature_delete:
                del patient_dict[patient_id][feature]

        ## Up to this point, we get the cleaned dataframe (train + test)
        patient_df = \
            pd.DataFrame.from_dict(patient_dict, orient='index')[
                list(feature_set)]

        ## missing value imputation using SoftImpute
        if self.verbose:
            print("[LOG]Impute missing value.")
        imputed_result = SoftImpute(verbose=False).complete(
            patient_df.drop(columns=["LOS", "OUT"]).values)
        imputed_patient_df = \
            pd.DataFrame(imputed_result, columns=patient_df.drop(
                columns=["LOS", "OUT"]).columns).set_index(
                patient_df.index.values)
        imputed_patient_df["LOS"] = patient_df["LOS"]
        imputed_patient_df["OUT"] = patient_df["OUT"]

        ## remove correlated features
        if remove_corr_feature:
            if self.verbose:
                print("[LOG]Remove correlated features.")
            dup_feature = self._corr_feature_filter(imputed_patient_df,
                                                    corr_thd=1)
            imputed_patient_df.drop(columns=dup_feature, inplace=True)
            if self.verbose:
                print("    Number of dumplicated featues:", len(dup_feature))

        # feature matrix preparation, train & test splitting
        train_id_list = list(id_set["train"])
        test_id_list = list(id_set["test"])

        ## remove rows that has infinite LoS
        if remove_inf_time_datapoint:
            if self.verbose:
                print("[LOG]Remove rows that has infinite LoS.")
            inf_id_set = \
                set(imputed_patient_df[
                        imputed_patient_df["LOS"] == np.inf].index)
            train_id_list = list(set(train_id_list) - inf_id_set)
            test_id_list = list(set(test_id_list) - inf_id_set)

        train_df = imputed_patient_df.loc[train_id_list]
        test_df = imputed_patient_df.loc[test_id_list]
        feature_list = np.array(train_df.drop(columns=["LOS", "OUT"]).columns)

        return train_df, test_df, feature_list

    def get_diseases_list(self):
        return self.SOURCES

    ######################### Helper Functions #################################

    def _get_file_path(self, source_index, filename):
        """
        Helper function, get the real path
        :param source_index: 0 -> "pancreatitis", 1 -> "ich", 2 -> "sepsis"
        :param filename: "train.csv", "test.csv", ...
        :return: an absolute file path
        """
        return self.DATA_PATH.format(SOURCE=self.SOURCES[source_index],
                                     FILENAME=filename)

    def _read_text_file(self, src_idx, prefix, tag, suffix):
        """
        Helper function. read text files generated by George.
        :param src_idx:
        :param prefix:
        :param tag:
        :param suffix:
        :return:
        """
        fname = "{PREFIX}{TAG}_{SUFFIX}.txt".format(PREFIX=prefix, TAG=tag,
                                                    SUFFIX=suffix)
        with open(self._get_file_path(src_idx, fname), "r") as txt_file:
            lines = txt_file.readlines()
            result_list = [float(line) for line in lines]
        return result_list

    def _remove_na_in_list(self, source_list):
        """
        helper function, remove NA from list
        :param source_list:
        :return:
        """
        return [item for item in source_list
                if not ((type(item) == float and np.isnan(item)) or \
                        (type(item) == str and item == "na") or \
                        item is None or item == "")]

    def _to_float_in_list(self, source_list):
        """
        helper function, parse a list of str into a list of float. only preserve
        those values that can be parsed
        :param source_list:
        :return:
        """
        output_list = []
        for item in source_list:
            try:
                output_list.append(float(item))
            except:
                pass
        return output_list

    def _pounds_to_kg(self, pounds):
        """
        Helper function，convert unit from pound to kg
        :param pounds:
        :return:
        """
        if type(pounds) == str:
            return pounds
        kilograms = pounds / 2.2
        return int(kilograms)

    def _check_same_distribution(self, list1, list2):
        """
        Helper function, check if two list of values comes from the same
        distribution (normal distribution in this case). This is used to merge
        two events that seem to be the same one.
        :param list1: list of values of the first event
        :param list2: list of values of the second event
        :return: boolean. true if the two list comes from the same dist.
        """
        list1_float = self._to_float_in_list(list1)
        list2_float = self._to_float_in_list(list2)

        n_1 = len(list1_float)
        n_2 = len(list2_float)

        if n_1 < 30 or n_2 < 30:
            return True

        mu_1 = np.mean(list1_float)
        mu_2 = np.mean(list2_float)
        std_1 = np.std(list1_float)
        std_2 = np.std(list2_float)

        z_score = (mu_1 - mu_2) / np.sqrt(std_1 ** 2 / n_1 + std_2 ** 2 / n_2)
        p_values = stats.norm.sf(abs(z_score)) * 2

        if p_values > 0.01:
            return True
        else:
            return False


if __name__ == '__main__':

    # Here is a demo to show how to use the FeatureEngineer class.
    # The same code can be found in the Util.py file as a helper func.
    # load_whole_data(dataset_idxs, verbose=False, data_path=None)

    dataset_idxs = [0, 1, 2]
    verbose = True

    fe = FeatureEngineer(verbose=verbose)

    # arbitrary parameters to filter dataset
    low_freq_event_thds = [0.02, 0.01, 0.003]
    low_freq_value_thds = [0.01, 0.005, 0.001]
    names_ref = fe.get_diseases_list()

    train_dfs = {}
    test_dfs = {}
    unique_times = {}
    dataset_names = []

    for dataset_idx in dataset_idxs:
        dataset_name = names_ref[dataset_idx]
        if verbose:
            print("current dataset:", dataset_name)
        train_df, test_df, feature_list = \
            fe.load_data(src_idx=dataset_idx, file_prefix="",
                low_freq_event_thd=low_freq_event_thds[dataset_idx],
                low_freq_value_thd=low_freq_value_thds[dataset_idx])
        train_dfs[dataset_name] = train_df
        test_dfs[dataset_name] = test_df
        unique_times[dataset_name] = sorted(list(train_df["LOS"].unique()))
        dataset_names.append(dataset_name)

    # return train_dfs, test_dfs, unique_times, dataset_names

